{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ea4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gdown\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from xml.etree import ElementTree\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, Video\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_data_path = 'train\\Annotations Boxes'\n",
    "train_data_path = 'train\\img-train'\n",
    "test_data_path = 'train\\img-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894390db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def select_files_in_folder(path):\n",
    "    for filename in os.listdir(path):\n",
    "        fullname = os.path.join(path, filename)\n",
    "        try:\n",
    "            tree = ET.parse(fullname)\n",
    "        except ET.ParseError:\n",
    "            print(f\"Error parsing {fullname}\")\n",
    "\n",
    "select_files_in_folder(\"train\\Annotations Boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20acfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_file_list = sorted([os.path.join(annot_data_path, i) for i in os.listdir(annot_data_path) if '.xml' in i])\n",
    "annot_file_list[:5], annot_file_list[-5:], len(annot_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_list = sorted([os.path.join(train_data_path, i) for i in os.listdir(train_data_path) if '.jpg' in i])\n",
    "train_file_list[:5], train_file_list[-5:], len(train_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_list = sorted([os.path.join(test_data_path, i) for i in os.listdir(test_data_path) if '.jpg' in i])\n",
    "test_file_list[:5], test_file_list[-5:], len(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b77785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_data_file_list = np.concatenate((train_file_list, test_file_list))\n",
    "image_data_file_list[:5], image_data_file_list[-5:], len(image_data_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b5755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting data from annotation files\n",
    "meta_list = [] # To store general info for every image\n",
    "object_list_train = [] # To store object classes info of train dataset\n",
    "object_list_test = [] # To store object classes info of test dataset\n",
    "\n",
    "for file in tqdm(annot_file_list):\n",
    "    meta_dict = {}\n",
    "    root = ElementTree.parse(file).getroot()\n",
    "    \n",
    "    # Filename - extracted\n",
    "    for path in image_data_file_list:\n",
    "        if root.find('filename').text in path:\n",
    "            meta_dict['filename'] = path\n",
    "            meta_dict['split_type'] = path.split(\"\\\\\")[1]\n",
    "    \n",
    "    # Width - extracted\n",
    "    meta_dict['width'] = int(root.find('size').find('width').text)\n",
    "    \n",
    "    # Height - extracted\n",
    "    meta_dict['height'] = int(root.find('size').find('height').text)\n",
    "    \n",
    "    # Objects - extracted and combined into a single string\n",
    "    meta_dict['objects'] = ', '.join(np.unique([obj.find('name').text for obj in root.findall('object')]))\n",
    "    meta_list.append(meta_dict)\n",
    "    \n",
    "    # Collecting all the object classes instance and counting total appearance\n",
    "    for obj in root.findall('object'):\n",
    "        if meta_dict['split_type'] == 'img-train':\n",
    "            object_list_train.append(obj.find('name').text)\n",
    "        elif meta_dict['split_type'] == 'img-test':\n",
    "            object_list_test.append(obj.find('name').text)\n",
    "    \n",
    "# Counting the instance for every object class\n",
    "object_instance_list_train = Counter(sorted(object_list_train))\n",
    "object_instance_list_test = Counter(sorted(object_list_test))\n",
    "    \n",
    "# Collecting Class list and indexing it also in a sequence\n",
    "class_dict = {k: v for v, k in enumerate(sorted(np.unique(object_list_train)))}\n",
    "meta_list[:5], object_instance_list_train, object_instance_list_test, class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rast_class_dict = class_dict[1:]\n",
    "# rast_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.DataFrame(meta_list)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df[meta_df['objects'] == 'roundabout_ahead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ce14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Size of the images width: {meta_df.width.unique()[0]} and height: {meta_df.height.unique()[0]}')\n",
    "print(f'Total number of classes with all possible combination: {len(meta_df.objects.unique())}')\n",
    "print(f'Total length of the training/validation dataset: {len(meta_df[meta_df[\"split_type\"] == \"img-train\"])} and testing dataset: {len(meta_df[meta_df[\"split_type\"] == \"img-test\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.objects.value_counts()[:27].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Top 27 Objects Classes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "meta_df[meta_df[\"split_type\"] == \"img-train\"].objects.value_counts()[:27].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Train/Val Dataset', fontsize=16)\n",
    "plt.subplot(1, 2, 2)\n",
    "meta_df[meta_df[\"split_type\"] == \"img-test\"].objects.value_counts()[:27].plot(kind='barh').invert_yaxis()\n",
    "plt.xlabel('Images (Count)')\n",
    "plt.title('Test Dataset', fontsize=16)\n",
    "plt.suptitle('Top 27 Objects Classes in the Dataset', fontsize=10, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(list(object_instance_list_train.keys()), list(object_instance_list_train.values()))\n",
    "plt.xlabel('Objects (Count)')\n",
    "plt.title('Train/Val Dataset', fontsize=16)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(list(object_instance_list_test.keys()), list(object_instance_list_test.values()))\n",
    "plt.xlabel('Objects (Count)')\n",
    "plt.title('Test Dataset', fontsize=16)\n",
    "plt.suptitle('Total Count of Object Instances Per Class in the Dataset', fontsize=20, fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec84a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_class = random.sample(meta_df.objects.tolist(), 1)[0]\n",
    "viz_list = meta_df[meta_df['objects'] == viz_class].filename.tolist()\n",
    "plt.figure(figsize=(20, 5))\n",
    "rand = random.sample(viz_list, 4)\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(plt.imread(rand[i]))\n",
    "    plt.suptitle(f'Objects in the Image: {viz_class}', fontsize=20, fontweight='bold')\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for extracting data\n",
    "def extract_data_from_xml(xml_file: str):\n",
    "    \"\"\"\n",
    "    A function to extract data like filename, size, classes and bboxes from xml file.\n",
    "    \n",
    "    Parameters: xml_file: str, A string containing the path to the file.\n",
    "    \n",
    "    Returns: data_dict: dict, A dict containing all the extracted data.\n",
    "    \"\"\"\n",
    "    root = ElementTree.parse(xml_file).getroot()\n",
    "    \n",
    "    # Creating dict and list to store data\n",
    "    data_dict = {}\n",
    "    data_dict['bboxes'] = []\n",
    "    \n",
    "    # Reading the xml file\n",
    "    for element in root:\n",
    "        # Getting the filename\n",
    "        if element.tag == 'filename':\n",
    "            data_dict['filename'] = element.text\n",
    "        \n",
    "        # Getting the image size\n",
    "        elif element.tag == 'size':\n",
    "            image_size = []\n",
    "            for size_element in element:\n",
    "                image_size.append(int(size_element.text))\n",
    "            data_dict['image_size'] = image_size\n",
    "        \n",
    "        # Getting the bounding box\n",
    "        elif element.tag == 'object':\n",
    "            bbox = {}\n",
    "            for obj_element in element:\n",
    "                # Object or Class name\n",
    "                if obj_element.tag == 'name':\n",
    "                    bbox['class'] = obj_element.text\n",
    "                # Object bounding box \n",
    "                elif obj_element.tag == 'bndbox':\n",
    "                    for bbox_element in obj_element:\n",
    "                        bbox[bbox_element.tag] = int(bbox_element.text)\n",
    "            data_dict['bboxes'].append(bbox)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = extract_data_from_xml(annot_file_list[2])\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66546a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_dict_to_yolo(data_dict: dict):\n",
    "    \"\"\"\n",
    "    A function to convert the extracted data dict into a text file as per the YOLO format.\n",
    "    The final text file is saved in the directory \"dior_data/yolo_annotations/data_dict['filename'].txt\".\n",
    "    \n",
    "    Parameters: data_dict: dict, A dict containing the data.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "   \n",
    "    for bbox in data_dict['bboxes']:\n",
    "        try:\n",
    "            class_id = class_dict[bbox['class']]\n",
    "        except KeyError:\n",
    "            print(f'Invalid Class. Object class: \"{bbox[\"class\"]}\" not present in the class list.')\n",
    "            \n",
    "        \n",
    "        img_w, img_h, _ = data_dict['image_size'] \n",
    "        \n",
    "        x_center = ((bbox['xmin'] + bbox['xmax']) / 2) / img_w\n",
    "        y_center = ((bbox['ymin'] + bbox['ymax']) / 2) / img_h\n",
    "        width = (bbox['xmax'] - bbox['xmin']) / img_w \n",
    "        height = (bbox['ymax'] - bbox['ymin']) / img_h\n",
    "        \n",
    "        \n",
    "        data.append(f'{class_id} {x_center:.3f} {y_center:.3f} {width:.3f} {height:.3f}')\n",
    "        \n",
    "    \n",
    "    yolo_annot_dir = os.path.join('train', 'yolo_annotations')\n",
    "    if not os.path.exists(yolo_annot_dir):\n",
    "        os.makedirs(yolo_annot_dir)\n",
    "    save_file_name = os.path.join(yolo_annot_dir, data_dict['filename'].replace('jpg','.txt'))\n",
    "    \n",
    "    \n",
    "    f = open(save_file_name, 'w+')\n",
    "    f.write('\\n'.join(data))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c844195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Annotation extraction and creation into Yolo has started.')\n",
    "for annot_file in tqdm(annot_file_list):\n",
    "    data_dict = extract_data_from_xml(annot_file)\n",
    "    convert_dict_to_yolo(data_dict)\n",
    "print('[INFO] All the annotation are converted into Yolo format.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d148c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extension_to_files(path, extension):\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith(extension):\n",
    "            os.rename(os.path.join(path, filename), os.path.join(path, filename + extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c81d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = 'train\\yolo_annotations'\n",
    "add_extension_to_files(input_folder_path, '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daacd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_annot_path = 'train\\yolo_annotations'\n",
    "yolo_annot_file_list = sorted([os.path.join(yolo_annot_path, i) for i in os.listdir(yolo_annot_path) if '.txt' in i])\n",
    "yolo_annot_file_list[:5], yolo_annot_file_list[-5:], len(yolo_annot_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7781366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict_idx = dict(zip(class_dict.values(), class_dict.keys()))\n",
    "class_dict_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f93c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bboxes(img_file: str, annot_file: str, class_dict: dict):\n",
    "    \"\"\"\n",
    "    A function to plot the bounding boxes amd their object classes onto the image.\n",
    "    \n",
    "    Parameters:\n",
    "        img_file: str, A string containing the path to the image file.\n",
    "        annot_file: str, A string containing the path to the annotation file in yolo format.\n",
    "        class_dict: dict, A dict containing the classes in the similar sequence as per the annot_file.\n",
    "    \"\"\"\n",
    "    # Reading the image and annot file\n",
    "    image = cv2.imread(img_file)\n",
    "    img_h, img_w, _ = image.shape\n",
    "    \n",
    "    with open(annot_file, 'r') as f:\n",
    "        data = f.read().split('\\n')\n",
    "        data = [i.split(' ') for i in data]\n",
    "        data = [[float(j) for j in i] for i in data]\n",
    "    \n",
    "    # Calculating the bbox in Pascal VOC format\n",
    "    for bbox in data:\n",
    "        class_idx, x_center, y_center, width, height = bbox\n",
    "        xmin = int((x_center - width / 2) * img_w)\n",
    "        ymin = int((y_center - height / 2) * img_h)\n",
    "        xmax = int((x_center + width / 2) * img_w)\n",
    "        ymax = int((y_center + height / 2) * img_h)\n",
    "        \n",
    "        # Correcting bbox if out of image size\n",
    "        if xmin < 0:\n",
    "            xmin = 0\n",
    "        if ymin < 0:\n",
    "            ymin = 0\n",
    "        if xmax > img_w - 1:\n",
    "            xmax = img_w - 1\n",
    "        if ymax > img_h - 1:\n",
    "            ymax = img_h - 1\n",
    "        \n",
    "        # Creating the box and label for the image\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 255, 0), 2)\n",
    "        cv2.putText(image, class_dict[class_idx], (xmin, 0 if ymin-10 < 0 else ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 0), 2)\n",
    "    \n",
    "    # Displaying the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba9f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "rand_int = random.sample(range(len(yolo_annot_file_list)), 3)\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plot_bboxes(image_data_file_list[rand_int[i]], yolo_annot_file_list[rand_int[i]], class_dict_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'datasets'\n",
    "image_dir = 'datasets/images'\n",
    "label_dir = 'datasets/labels'\n",
    "img_train_dir = 'datasets/images/train'\n",
    "img_val_dir = 'datasets/images/val'\n",
    "label_train_dir = 'datasets/labels/train'\n",
    "label_val_dir = 'datasets/labels/val'\n",
    "\n",
    "\n",
    "total_val_size = int(len(test_file_list) * 0.2)\n",
    "total_val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(img_train_dir):\n",
    "    os.makedirs(img_train_dir)\n",
    "\n",
    "for filepath in tqdm(train_file_list):\n",
    "    if os.path.isfile(filepath):\n",
    "        shutil.move(filepath, img_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbffa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(img_val_dir):\n",
    "    os.makedirs(img_val_dir)\n",
    "\n",
    "for filepath in tqdm(test_file_list[:total_val_size]):\n",
    "    if os.path.isfile(filepath):\n",
    "        shutil.move(filepath, img_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(label_train_dir):\n",
    "    os.makedirs(label_train_dir)\n",
    "\n",
    "for filepath in tqdm(train_file_list):\n",
    "    file_path = os.path.join('train/yolo_annotations', filepath.replace('jpg', 'txt').split('\\\\')[-1])\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.move(file_path, label_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d684bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(label_val_dir):\n",
    "    os.makedirs(label_val_dir)\n",
    "\n",
    "for filepath in tqdm(test_file_list[:total_val_size]):\n",
    "    file_path = os.path.join('train/yolo_annotations', filepath.replace('jpg', 'txt').split('\\\\')[-1])\n",
    "    if os.path.isfile(file_path):\n",
    "        shutil.move(file_path, label_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565e497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile road.yaml\n",
    "# Path to my dataset\n",
    "path: 'C:/Users/COMPUTER-JOKAR/Soil Mechanics Laboratory/Mechanics Laboratory/datasets'\n",
    "train: 'images/train'\n",
    "val: 'images/val'\n",
    "\n",
    "# Classes as per mentioned in the annotation\n",
    "names:\n",
    "    0: bridge\n",
    "    1: concrete\n",
    "    2: guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa63d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de25e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.train(data='road.yaml', epochs=50, batch=15, name='yolov8n_epochs50_batch16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test_dir = 'datasets/images/test'\n",
    "\n",
    "# Moving the Testing images[80% of test dataset]\n",
    "if not os.path.exists(img_test_dir):\n",
    "    os.makedirs(img_test_dir)\n",
    "\n",
    "test_list = os.listdir('train/img-test')\n",
    "for filename in tqdm(test_list):\n",
    "    filepath = os.path.join('train/img-test', filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        shutil.move(filepath, img_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test_dir = 'datasets/labels/test'\n",
    "\n",
    "# Moving the testing annotation [80% of testing dataset]\n",
    "if not os.path.exists(label_test_dir):\n",
    "    os.makedirs(label_test_dir)\n",
    "\n",
    "for filename in tqdm(test_list):\n",
    "    filepath = os.path.join('train/yolo_annotations', filename).replace('jpg', 'txt')\n",
    "    if os.path.isfile(filepath):\n",
    "        shutil.move(filepath, label_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3951a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_road.yaml\n",
    "# Path to my dataset\n",
    "path: 'C:/Users/COMPUTER-JOKAR/Soil Mechanics Laboratory/Mechanics Laboratory/datasets'\n",
    "train: \n",
    "val: 'images/test'\n",
    "\n",
    "# Classes as per mentioned in the annotation\n",
    "names:\n",
    "    0: bridge\n",
    "    1: concrete\n",
    "    2: guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = YOLO('runs/detect/yolov8n_epochs100_batch16/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13461976",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = my_model.predict(source='/Users/COMPUTER-JOKAR/Soil Mechanics Laboratory/Mechanics Laboratory/sign_data/train/', save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.val(data='test_road.yaml', imgsz=800, name='yolov8n_val_on_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae55509",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('runs\\detect\\yolov8n_val_on_test5\\confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('runs\\detect\\yolov8n_val_on_test5\\F1_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('runs\\detect\\yolov8n_val_on_test5\\PR_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result_image = ['runs/detect/yolov8n_val_on_test5/val_batch0_labels.jpg', 'runs/detect/yolov8n_val_on_test5/val_batch0_pred.jpg', 'runs/detect/yolov8n_val_on_test5/val_batch1_labels.jpg', 'runs/detect/yolov8n_val_on_test5/val_batch1_pred.jpg', 'runs/detect/yolov8n_val_on_test5/val_batch2_labels.jpg', 'runs/detect/yolov8n_val_on_test5/val_batch2_pred.jpg']\n",
    "plt.figure(figsize=(20, 30))\n",
    "for i in range(len(val_result_image)):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.imshow(plt.imread(val_result_image[i]))\n",
    "    plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_epochs50_batch164/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f11da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test_dir = 'datasets/images/test'\n",
    "label_test_dir = 'datasets/labels/test'\n",
    "rand_img = random.sample(os.listdir(img_test_dir), 1)[0]\n",
    "rand_img_path = os.path.join(img_test_dir, rand_img)\n",
    "rand_label_path = os.path.join(label_test_dir, rand_img).replace('jpg', 'txt')\n",
    "\n",
    "# Predicting the object using the yolo model\n",
    "pred_list = model.predict(source=rand_img_path, imgsz=800, save=True, conf=0.5)\n",
    "pred_img_path = os.path.join('runs\\detect\\predict', rand_img) # Predict path can change\n",
    "\n",
    "# Ploting a the true and predicted images with bounding boxes\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_bboxes(rand_img_path, rand_label_path, class_dict_idx)\n",
    "plt.title('Original Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(plt.imread(pred_img_path))\n",
    "plt.title('Predicted Image')\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2e729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(source='datasets/images/test', save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = ['test_data/bridge_test.mp4', 'test_data/concrete_test.mp4']\n",
    "for file in video_list:\n",
    "    pred_list = model.predict(source=file, imgsz=800, save=True, conf=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('runs/detect/predict2/bridge_test.mp4', width=500, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d2b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('runs/detect/predict2/concrete_test.mp4', width=500, embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
